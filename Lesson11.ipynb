{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "annual-sherman",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Библиотека torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-fiction",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Установка\n",
    "```bash\n",
    "pip install torch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-remark",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Математика"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "embedded-luxembourg",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rotary-promise",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000, 3.0000, 5.0000, 2.0000, 1.3000])\n",
      "tensor([-3.1416, -2.8109, -2.4802, -2.1495, -1.8188, -1.4881, -1.1574, -0.8267,\n",
      "        -0.4960, -0.1653,  0.1653,  0.4960,  0.8267,  1.1574,  1.4881,  1.8188,\n",
      "         2.1495,  2.4802,  2.8109,  3.1416])\n",
      "tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000, 2.5000, 3.0000, 3.5000, 4.0000,\n",
      "        4.5000, 5.0000, 5.5000, 6.0000, 6.5000, 7.0000, 7.5000, 8.0000, 8.5000,\n",
      "        9.0000, 9.5000])\n",
      "tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([1, 3, 5, 2, 1.3])\n",
    "print(x)\n",
    "x = torch.linspace(-np.pi, np.pi, 20, dtype=torch.float32)\n",
    "print(x)\n",
    "x = torch.arange(0, 10, 0.5, dtype=torch.float32)\n",
    "print(x)\n",
    "x = torch.ones(2, 1, 3, dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caroline-reading",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288],\n",
      "        [ 0.2345,  0.2303],\n",
      "        [-1.1229, -0.1863]])\n",
      "tensor([[ 2.2082, -0.6380],\n",
      "        [ 0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094]])\n",
      "tensor([[ 1.1103, -1.6898, -0.9890,  0.9580],\n",
      "        [ 1.3221,  0.8172, -0.7658, -0.7506]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.randn((3, 2))\n",
    "b = torch.randn((3, 2))\n",
    "c = torch.randn((2, 4))\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nervous-photography",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2128,  0.5358],\n",
      "        [ 0.4476,  0.3018],\n",
      "        [-0.8367,  0.4687]])\n",
      "tensor([[ 0.3304,  0.1285],\n",
      "        [ 0.2323,  0.2283],\n",
      "        [-0.9013, -0.1853]])\n",
      "tensor([[ 0.4444, -0.1187,  0.5496,  0.5752],\n",
      "        [ 0.2461,  0.6843,  0.7208,  0.7313]])\n",
      "tensor([[  3,   1],\n",
      "        [  2,   2],\n",
      "        [-11,  -1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(a + b ** 2)\n",
    "print(torch.sin(a))\n",
    "print(torch.cos(c))\n",
    "print((a * 10).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-laugh",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Основное оличие от numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "educated-enclosure",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None None\n",
      "tensor([[0.6343, 0.3644, 0.7104],\n",
      "        [0.9464, 0.7890, 0.2814]], requires_grad=True)\n",
      "tensor([[0.7886, 0.5895, 0.7539],\n",
      "        [0.1952, 0.0050, 0.3068]])\n"
     ]
    }
   ],
   "source": [
    "# До выполнения операций градиенты не заданы\n",
    "a = torch.rand(2, 3, dtype=torch.float32, requires_grad=True)\n",
    "x = torch.rand(2, 3, dtype=torch.float32, requires_grad=False)\n",
    "y = torch.rand(2, 3, dtype=torch.float32, requires_grad=False)\n",
    "print(a.grad, x.grad, y.grad)\n",
    "print(a)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conscious-tiffany",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9700, -0.8347,  0.8219],\n",
      "        [ 0.7421, -0.1122, -0.4314]]) None None\n"
     ]
    }
   ],
   "source": [
    "y_pred = a ** 2 + x ** 3\n",
    "loss = (y_pred - y).pow(2).sum()\n",
    "loss.backward()\n",
    "print(a.grad, x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-conflict",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Вычисления на GPU\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/cudagraphs-pytorch.png\" alt=\"CUDA\" width=30% height=30%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-madness",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://habrastorage.org/r/w1560/getpro/habr/post_images/5e2/048/3f5/5e20483f59e87b0a395b0fae0e6495c5.png\" alt=\"CUDAplot\" width=80% height=80%>\n",
    "\n",
    "**FLOPS** (FLoating-point Operations Per Second) — внесистемная единица, используемая для измерения производительности компьютеров, показывающая, сколько операций с плавающей запятой в секунду выполняет данная вычислительная система."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-wrestling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Архитектура CPU\n",
    "<img src=\"https://habrastorage.org/r/w1560/getpro/habr/post_images/df0/8c2/4c3/df08c24c3fe92cd97356670729c318cd.png\" alt=\"CUDAplot\" width=40% height=40%>\n",
    "ALU (Арифметико-логическое устройство) — блок процессора, который под управлением устройства управления служит для выполнения арифметических и логических преобразований (начиная от элементарных) над данными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-wound",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Архитектура GPU\n",
    "<img src=\"https://habrastorage.org/r/w1560/getpro/habr/post_images/0fe/138/0cc/0fe1380ccbb321b289d16e39a499009a.png\" alt=\"CUDAplot\" width=40% height=40%>\n",
    "\n",
    "Архитектура ядра GPU и логических элементов существенно проще, чем на CPU, а именно, отсутствуют Momory pre-fetcher, Branch predictor и прочие вспомогательные блоки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rental-merit",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение, доступна ли CUDA\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-corruption",
   "metadata": {},
   "source": [
    "```python\n",
    "x = torch.Tensor([1, 3, 5, 2, 1.3], device='cpu')\n",
    "x = torch.Tensor([1, 3, 5, 2, 1.3], device='cuda:0')\n",
    "```\n",
    "или\n",
    "```python\n",
    "device = torch.device('cuda:0')\n",
    "x = torch.Tensor([1, 3, 5, 2, 1.3], device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-cruise",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "behavioral-interview",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Список методов:\n",
      " ['add_module', 'apply', 'bfloat16', 'buffers', 'children', 'cpu', 'cuda', 'double', 'eval', 'extra_repr', 'float', 'forward', 'half', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'requires_grad_', 'share_memory', 'state_dict', 'to', 'train', 'type', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Родительский класс для всех моделей и их элементов\n",
    "nn.Module\n",
    "\n",
    "method_list = [method for method in dir(nn.Module) if not method.startswith('_') and callable(getattr(nn.Module, method))]\n",
    "print(\"Список методов:\\n\", method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "advised-moscow",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Простая модель\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\"Регистрация блоков\"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_ch, 32)  # Полносвязный слой 1\n",
    "        self.fc2 = nn.Linear(32, out_ch, bias=False)  # Полносвязный слой 2\n",
    "        self.relu = nn.ReLU()  # Функция активации\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Прямой проход\"\"\"\n",
    "        h = self.fc1(x)\n",
    "        h = self.relu(h)\n",
    "        h = self.fc2(h)\n",
    "        y = self.relu(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "devoted-filename",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SimpleModel(\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "FC1: Linear(in_features=64, out_features=32, bias=True)\n",
      "\n",
      "Weight: torch.Size([32, 64]) \n",
      " Parameter containing:\n",
      "tensor([[ 0.0978, -0.0888,  0.0079,  ..., -0.0748,  0.0003, -0.0465],\n",
      "        [-0.0087, -0.0847, -0.0858,  ..., -0.0445,  0.0652,  0.0657],\n",
      "        [ 0.0467, -0.0220, -0.0331,  ...,  0.0415, -0.0371,  0.0772],\n",
      "        ...,\n",
      "        [-0.1192,  0.0228,  0.0171,  ...,  0.0446, -0.1053, -0.0153],\n",
      "        [ 0.0475,  0.1020,  0.0376,  ...,  0.0884,  0.0099, -0.0895],\n",
      "        [-0.0947, -0.0487,  0.0364,  ..., -0.0513, -0.0247,  0.0608]],\n",
      "       requires_grad=True)\n",
      "\n",
      "Weight: torch.Size([32]) \n",
      " Parameter containing:\n",
      "tensor([-0.0029,  0.1069, -0.0072, -0.0179,  0.0706, -0.0032, -0.0249,  0.0418,\n",
      "         0.0986,  0.0312,  0.0870,  0.0810,  0.0353,  0.0435,  0.0403,  0.0130,\n",
      "         0.1063,  0.1154,  0.0445, -0.0062,  0.1121, -0.0973,  0.0918,  0.0662,\n",
      "         0.0803, -0.0096, -0.0933,  0.0892, -0.0547,  0.0329,  0.0708, -0.1168],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleModel(64, 10)\n",
    "\n",
    "print('Model:', model)\n",
    "print('FC1:', model.fc1)\n",
    "print()\n",
    "print('Weight:', model.fc1.weight.shape, '\\n', model.fc1.weight)\n",
    "print()\n",
    "print('Weight:', model.fc1.bias.shape, '\\n', model.fc1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "blind-dinner",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0978, -0.0888,  0.0079,  ..., -0.0748,  0.0003, -0.0465],\n",
       "         [-0.0087, -0.0847, -0.0858,  ..., -0.0445,  0.0652,  0.0657],\n",
       "         [ 0.0467, -0.0220, -0.0331,  ...,  0.0415, -0.0371,  0.0772],\n",
       "         ...,\n",
       "         [-0.1192,  0.0228,  0.0171,  ...,  0.0446, -0.1053, -0.0153],\n",
       "         [ 0.0475,  0.1020,  0.0376,  ...,  0.0884,  0.0099, -0.0895],\n",
       "         [-0.0947, -0.0487,  0.0364,  ..., -0.0513, -0.0247,  0.0608]],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.0029,  0.1069, -0.0072, -0.0179,  0.0706, -0.0032, -0.0249,  0.0418,\n",
       "          0.0986,  0.0312,  0.0870,  0.0810,  0.0353,  0.0435,  0.0403,  0.0130,\n",
       "          0.1063,  0.1154,  0.0445, -0.0062,  0.1121, -0.0973,  0.0918,  0.0662,\n",
       "          0.0803, -0.0096, -0.0933,  0.0892, -0.0547,  0.0329,  0.0708, -0.1168],\n",
       "        requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[ 0.0117, -0.0628,  0.1255, -0.1126,  0.1073,  0.1102,  0.0975,  0.1697,\n",
       "           0.0268, -0.1602,  0.1262,  0.0162,  0.0035, -0.1684, -0.0397, -0.0858,\n",
       "           0.1731,  0.1259,  0.1319,  0.1211, -0.1096, -0.0759, -0.0692, -0.0709,\n",
       "           0.1376,  0.0601,  0.1417, -0.0083, -0.1576, -0.0513,  0.1242,  0.1076],\n",
       "         [ 0.0547, -0.0519, -0.1650, -0.0950,  0.0922, -0.0713,  0.0846,  0.0483,\n",
       "          -0.0679,  0.1077, -0.0568, -0.0059, -0.0673, -0.1677,  0.0285,  0.0511,\n",
       "          -0.0890,  0.1084, -0.1586, -0.1704, -0.1092, -0.0611, -0.0196, -0.0342,\n",
       "           0.0807, -0.1551,  0.0260,  0.0031,  0.0467,  0.1757,  0.1088, -0.0979],\n",
       "         [-0.1449, -0.0428, -0.0887,  0.0617,  0.0016,  0.1422,  0.0694, -0.0325,\n",
       "           0.0061,  0.0423, -0.0219,  0.1426, -0.1382,  0.0862, -0.1127, -0.1549,\n",
       "          -0.0363,  0.0861,  0.1249,  0.0444, -0.0153, -0.0099, -0.0717, -0.0159,\n",
       "          -0.0122,  0.1504,  0.1035, -0.1415,  0.0007, -0.1087, -0.0281, -0.1405],\n",
       "         [ 0.0460, -0.1275,  0.1745,  0.1335, -0.1217, -0.1664,  0.0069, -0.0493,\n",
       "           0.0519, -0.1197,  0.0724, -0.1527, -0.0933, -0.0929,  0.0855, -0.0046,\n",
       "          -0.1699, -0.1010, -0.0818, -0.1640,  0.0226,  0.0630, -0.0324, -0.0703,\n",
       "          -0.0011, -0.1404, -0.1076, -0.0040,  0.1570, -0.0544,  0.0876, -0.0696],\n",
       "         [-0.0471, -0.1683,  0.1465,  0.1721, -0.0037,  0.1123,  0.1446,  0.0878,\n",
       "           0.0910, -0.1683, -0.1661, -0.1357,  0.0553, -0.1529, -0.1349, -0.0857,\n",
       "          -0.1629,  0.0002,  0.0255, -0.1034,  0.1157, -0.0671, -0.0498,  0.1347,\n",
       "           0.1518, -0.0203, -0.1269, -0.0887,  0.0225,  0.1036,  0.0880, -0.0781],\n",
       "         [-0.1671, -0.1547, -0.0562,  0.0022, -0.0794, -0.1398,  0.0186, -0.1403,\n",
       "          -0.1358, -0.1437, -0.0479,  0.1561,  0.0691, -0.0210,  0.0364,  0.0704,\n",
       "           0.0010, -0.1426, -0.1089, -0.1201, -0.0045, -0.1618, -0.0321, -0.0646,\n",
       "           0.1735,  0.1437,  0.1027,  0.0099, -0.0572,  0.0451, -0.1221, -0.0754],\n",
       "         [-0.0355, -0.0415, -0.1675,  0.0703,  0.1456, -0.0975,  0.0477, -0.0205,\n",
       "          -0.0064,  0.0103,  0.1532,  0.0237, -0.1118, -0.0636,  0.0864,  0.0552,\n",
       "           0.0034, -0.0233, -0.0688, -0.0627, -0.0932,  0.0290,  0.0496,  0.1354,\n",
       "          -0.0399,  0.0127,  0.1142,  0.0769,  0.0227,  0.0543,  0.0092, -0.1516],\n",
       "         [ 0.0068, -0.0386,  0.1595,  0.1596,  0.0351,  0.1580, -0.0910,  0.1536,\n",
       "           0.1522, -0.0068, -0.1419, -0.0570, -0.0473,  0.0029, -0.1603,  0.0328,\n",
       "          -0.0735, -0.0453, -0.0913, -0.0373, -0.1642, -0.0937,  0.0163,  0.1158,\n",
       "           0.1606, -0.1337,  0.0742, -0.0996, -0.1367, -0.0078, -0.1014, -0.1522],\n",
       "         [-0.1652, -0.1183,  0.1453,  0.0986, -0.0700, -0.1183, -0.1360, -0.1044,\n",
       "          -0.0727,  0.1062,  0.0579, -0.0681,  0.1217, -0.1098, -0.1440,  0.0386,\n",
       "          -0.0975,  0.0503,  0.1498, -0.0836, -0.1234,  0.0665, -0.1597,  0.1577,\n",
       "          -0.0835,  0.0855, -0.1406, -0.1197,  0.0195,  0.0628,  0.1371,  0.1478],\n",
       "         [-0.0102, -0.0227,  0.0292, -0.0411,  0.0060, -0.1674, -0.1451,  0.1211,\n",
       "          -0.1033,  0.1392,  0.0194,  0.0431,  0.1607, -0.0983, -0.0279,  0.0313,\n",
       "           0.0604, -0.1720,  0.0628, -0.1042,  0.0232,  0.1196, -0.0176, -0.0056,\n",
       "          -0.0132,  0.1475,  0.1005,  0.0297, -0.1064, -0.0960,  0.1408,  0.1272]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-computer",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Проход модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demonstrated-earthquake",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0899],\n",
       "        [0.1487, 0.0000, 0.0328, 0.0000, 0.0000, 0.0000, 0.0132, 0.0000, 0.0000,\n",
       "         0.0056],\n",
       "        [0.1124, 0.0000, 0.1217, 0.0000, 0.0000, 0.0000, 0.0245, 0.0000, 0.0260,\n",
       "         0.0228],\n",
       "        [0.1245, 0.0000, 0.0407, 0.0000, 0.0000, 0.0000, 0.0208, 0.0000, 0.0000,\n",
       "         0.0340]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(4, 64)  # batch size = 4\n",
    "y = torch.rand(4, 10)\n",
    "\n",
    "w1_1 = model.fc1.weight.data.clone()  # Сохранение состояния весов\n",
    "\n",
    "y_pred = model(x)  # Прямой проход\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "previous-solution",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.6101, grad_fn=<L1LossBackward>)\n",
      "Grad before: None\n",
      "Grad after: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0058,  0.0047,  0.0101,  ...,  0.0026,  0.0090,  0.0050],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0046, -0.0006, -0.0052,  ..., -0.0026, -0.0051, -0.0006]])\n"
     ]
    }
   ],
   "source": [
    "# Функция потерь L1 (MAE)\n",
    "l1_loss = nn.L1Loss()\n",
    "loss = l1_loss(y, y_pred)\n",
    "print('Loss:', loss)\n",
    "\n",
    "print('Grad before:', model.fc1.weight.grad)\n",
    "loss.backward()  # Обратный проход\n",
    "print('Grad after:', model.fc1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "noble-footage",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "w1_2 = model.fc1.weight.data.clone()\n",
    "print(w1_2 - w1_1)  # Веса не изменились"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-grade",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обновление весов \n",
    "\n",
    "Для обновления весов в модели используются оптимизаторы:  \n",
    "  \n",
    "* SGD (Stochastic Gradient Descent) для оптимизации импульса.\n",
    "* RMSprop – адаптивная оптимизация скорости обучения по методу Джеффа Хинтона.\n",
    "* Adam – адаптивная оценка моментов, которая также использует адаптивную скорость обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "colonial-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание оптимизатора\n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eastern-worth",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "juvenile-processor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-5.8291e-06, -4.6641e-06, -1.0058e-05,  ..., -2.6152e-06,\n",
      "         -8.9854e-06, -4.9695e-06],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 4.6343e-06,  6.4820e-07,  5.2191e-06,  ...,  2.5630e-06,\n",
      "          5.0627e-06,  6.2957e-07]])\n"
     ]
    }
   ],
   "source": [
    "w1_3 = model.fc1.weight.data.clone()\n",
    "print(w1_3 - w1_2)  # Веса обновились"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "copyrighted-burning",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0058,  0.0047,  0.0101,  ...,  0.0026,  0.0090,  0.0050],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0046, -0.0006, -0.0052,  ..., -0.0026, -0.0051, -0.0006]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Градиенты всё ещё содержат старые значения, потому при следующем вычислении они будут учитываться\n",
    "model.fc1.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "large-signal",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Чтобы обнулить градиенты, используем метод zero_grad() у оптимизатора\n",
    "opt.zero_grad()\n",
    "model.fc1.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-relation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch\n",
    "Batch (пакет) – количество обучающих примеров за одну итерацию. Чем больше batch size, тем больше места будет необходимо. Если batch size маленький, то изменение весов будет подстраиваться под отдельные примеры, а не под общие тенденции."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-charleston",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "perceived-pathology",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "distinct-recipient",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorDataset(data.Dataset):\n",
    "    def __init__(self, in_size, out_size, num_samples, func='sin'):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.func = func\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.rand(self.in_size)\n",
    "        if self.func == 'sin':\n",
    "            x = torch.sin(x)\n",
    "        elif self.func == 'cos':\n",
    "            x = torch.cos(x)\n",
    "        y = x[:self.out_size].clone()\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "willing-tribune",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64]) torch.Size([16, 10])\n"
     ]
    }
   ],
   "source": [
    "dataset = GeneratorDataset(64, 10, 128)\n",
    "dataloader = data.DataLoader(dataset, batch_size=16)\n",
    "for x, y in dataloader:\n",
    "    break\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-pharmacology",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "virtual-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(64, 10)\n",
    "l1_loss = nn.L1Loss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exact-status",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.44412118196487427\n",
      "Loss 0.39865168929100037\n",
      "Loss 0.4141206741333008\n",
      "Loss 0.3751830458641052\n",
      "Loss 0.39952030777931213\n",
      "Loss 0.3894100785255432\n",
      "Loss 0.35259371995925903\n",
      "Loss 0.37724384665489197\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataloader:\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    y_pred = model(x)\n",
    "    loss = l1_loss(y, y_pred)\n",
    "    loss.backward()\n",
    "    print('Loss', loss.item())\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-casting",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Пример модели UNet\n",
    "  \n",
    "[Репозиторий](https://github.com/milesial/Pytorch-UNet)  \n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/41ded1456b9dbe13b8d73d8da539dac95cb8aa721ebe5fb798af732ca9f04c92/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67\" alt=\"UNet\" width=80% height=80%>\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
